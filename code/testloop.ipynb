{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install scikit-learn\n",
    "# %pip install tensorflow\n",
    "# %pip install seaborn matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3662 images belonging to 2 classes.\n",
      "Found 915 images belonging to 2 classes.\n",
      "Found 4577 images belonging to 2 classes.\n",
      "Found 2745 images belonging to 2 classes.\n",
      "Found 916 images belonging to 2 classes.\n",
      "Found 916 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1726751128.017363      40 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726751128.054071      40 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726751128.054213      40 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726751128.055340      40 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726751128.055452      40 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726751128.055515      40 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726751128.111720      40 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726751128.111842      40 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726751128.111916      40 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-19 13:05:28.112357: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3479 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m74836368/74836368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 0us/step\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1726751147.494799     217 service.cc:146] XLA service 0x7b7b0c003240 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1726751147.494873     217 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce GTX 1050 Ti, Compute Capability 6.1\n",
      "2024-09-19 13:05:47.999169: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-09-19 13:05:50.094706: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8906\n",
      "2024-09-19 13:06:02.312050: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.53GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/10\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:16\u001b[0m 35s/step - accuracy: 0.3750 - loss: 0.8273"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1726751172.938586     217 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 4s/step - accuracy: 0.5178 - loss: 0.7238 - val_accuracy: 0.6812 - val_loss: 0.5712\n",
      "Epoch 2/30\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 835ms/step - accuracy: 0.5912 - loss: 0.6429 - val_accuracy: 0.8024 - val_loss: 0.4840\n",
      "Epoch 3/30\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 836ms/step - accuracy: 0.7020 - loss: 0.5864 - val_accuracy: 0.6703 - val_loss: 0.5635\n",
      "Epoch 4/30\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 837ms/step - accuracy: 0.7421 - loss: 0.5293 - val_accuracy: 0.8548 - val_loss: 0.4107\n",
      "Epoch 5/30\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 836ms/step - accuracy: 0.7875 - loss: 0.4542 - val_accuracy: 0.8548 - val_loss: 0.3903\n",
      "Epoch 6/30\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 840ms/step - accuracy: 0.7295 - loss: 0.5508 - val_accuracy: 0.7838 - val_loss: 0.4447\n",
      "Epoch 7/30\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.7633 - loss: 0.5028"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Dense, Flatten\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Enable mixed precision training\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "clear_session()  # This frees the GPU memory, peventing training deadlock\n",
    "\n",
    "# Function to create directories\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "for setClass in ['metal','glass', 'trash', 'cardboard', 'paper', 'plastic']:\n",
    "    clear_session()\n",
    "    global classez\n",
    "    classez = [setClass, 'other']\n",
    "    # data path\n",
    "    global dataset_path\n",
    "    dataset_path = f'/app/data/Datasets/augmented_{classez[0]}_and_{classez[1]}'  # <- docker file path      # \"G:\\My Drive\\Datasets\\ \"\n",
    "\n",
    "    tar = (224, 224)\n",
    "    batch = 16 # 16\n",
    "    #Split the dataset\n",
    "    # Create an ImageDataGenerator for each subset\n",
    "    datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)  # 20% for validation\n",
    "\n",
    "    # Training data generator\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        dataset_path,\n",
    "        target_size=tar,\n",
    "        batch_size=batch,\n",
    "        class_mode='binary',\n",
    "        subset='training')  # Set as training data\n",
    "\n",
    "    # Validation data generator\n",
    "    validation_generator = datagen.flow_from_directory(\n",
    "        dataset_path,\n",
    "        target_size=tar,\n",
    "        batch_size=batch,\n",
    "        class_mode='binary',\n",
    "        subset='validation')  # Set as validation data\n",
    "\n",
    "    # Create a separate ImageDataGenerator for test data\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Test data generator\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        dataset_path,\n",
    "        target_size=tar,\n",
    "        batch_size=batch,\n",
    "        class_mode='binary')  # No subset for testing\n",
    "    \n",
    "   \n",
    "\n",
    "    # Paths to save splits\n",
    "    base_dir = f'/app/data/Datasets/augmented_{classez[0]}_and_{classez[1]}-split'  # \"G:\\My Drive\\Datasets\\Trashnet-resized-split\"\n",
    "    train_dir = os.path.join(base_dir, 'train')\n",
    "    val_dir = os.path.join(base_dir, 'val')\n",
    "    test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "    create_dir(base_dir)\n",
    "    create_dir(train_dir)\n",
    "    create_dir(val_dir)\n",
    "    create_dir(test_dir)\n",
    "\n",
    "    for class_name in classez: #, 'cardboard', 'metal', 'paper', 'plastic', 'trash']:\n",
    "        # Create class directories\n",
    "        create_dir(os.path.join(train_dir, class_name))\n",
    "        create_dir(os.path.join(val_dir, class_name))\n",
    "        create_dir(os.path.join(test_dir, class_name))\n",
    "\n",
    "        # List all files in the class directory\n",
    "        class_files = os.listdir(os.path.join(dataset_path, class_name))\n",
    "        train_files, test_files = train_test_split(class_files, test_size=0.2, random_state=42)  # 20% for testing\n",
    "        train_files, val_files = train_test_split(train_files, test_size=0.25, random_state=42)  # 20% of remaining for validation (0.25 * 0.8 = 0.2)\n",
    "\n",
    "        # Copy files to respective directories\n",
    "        for file in train_files:\n",
    "            shutil.copy(os.path.join(dataset_path, class_name, file), os.path.join(train_dir, class_name, file))\n",
    "\n",
    "        for file in val_files:\n",
    "            shutil.copy(os.path.join(dataset_path, class_name, file), os.path.join(val_dir, class_name, file))\n",
    "\n",
    "        for file in test_files:\n",
    "            shutil.copy(os.path.join(dataset_path, class_name, file), os.path.join(test_dir, class_name, file))\n",
    "        \n",
    "    train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=tar,\n",
    "    batch_size=batch,\n",
    "    class_mode='binary'\n",
    "    # ,color_mode='grayscale'\n",
    "    )\n",
    "\n",
    "    validation_generator = datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=tar,\n",
    "        batch_size=batch,\n",
    "        class_mode='binary'\n",
    "        # ,color_mode='grayscale'\n",
    "        )\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=tar,\n",
    "        batch_size=batch,\n",
    "        class_mode='binary'\n",
    "        # ,color_mode='grayscale'\n",
    "        )\n",
    "\n",
    "    img_shape = (224,224,3)\n",
    "    img_size = tar\n",
    "    batch_size = batch #16 #32\n",
    "    num_classes = 2\n",
    "    log_file = '/app/data/code/accuracy_log7.csv'\n",
    "\n",
    "    # DenseNet201 # MobileNetV3 # EfficientNetB0\n",
    "    base_model = tf.keras.applications.DenseNet201(input_shape=img_shape,\n",
    "                                                include_top=False,\n",
    "                                                weights='imagenet')\n",
    "\n",
    "    model_name = base_model.name #'EfficientNetB0' # 'DenseNet201' # 'MobileNetV2'\n",
    "    # print(model_name)\n",
    "\n",
    "    base_model.trainable = False\n",
    "    for layer in base_model.layers[-6:]:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    class AccuracyLogger(Callback):\n",
    "        def __init__(self,   log_file, model_name, img_shape, batch_size, num_classes):\n",
    "            super().__init__()\n",
    "            self.log_file = log_file\n",
    "            self.model_name = model_name\n",
    "            self.img_shape = img_shape\n",
    "            self.batch_size = batch_size\n",
    "            self.num_classes = num_classes\n",
    "            \n",
    "            # self.summary = summary\n",
    "\n",
    "            # Write model and dataset information at the beginning of the log file\n",
    "            with open(self.log_file, 'a') as f:\n",
    "                f.write(f\"\\nModel: {self.model_name}\\n\")\n",
    "                # f.write(f\"Model summary:{self.summary}\\n\")\n",
    "                f.write(f\"Image shape: {self.img_shape}\\n\")\n",
    "                f.write(f\"Batch size: {self.batch_size}\\n\")\n",
    "                f.write(f\"Dataset: {dataset_path}\\n\")\n",
    "                f.write(f\"Number of classes: {self.num_classes}\\n\\n\")\n",
    "                f.write('Epoch,Accuracy,Validation Accuracy, Loss, Validation Loss\\n')\n",
    "                f.flush()\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            logs = logs or {}\n",
    "            accuracy = logs.get('accuracy')\n",
    "            val_accuracy = logs.get('val_accuracy')\n",
    "            loss = logs.get('loss')\n",
    "            val_loss = logs.get('val_loss')\n",
    "\n",
    "            # Write the epoch, accuracy, and validation accuracy to the log file\n",
    "            with open(self.log_file, 'a') as f:\n",
    "                f.write(f\"{epoch + 1},{accuracy},{val_accuracy},{loss},{val_loss}\\n\")\n",
    "                f.flush()\n",
    "\n",
    "    model = tf.keras.Sequential([base_model,\n",
    "                                #  tf.keras.layers.Flatten(),\n",
    "                                tf.keras.layers.GlobalAveragePooling2D(),\n",
    "                                #  tf.keras.layers.Dense(batch, activation=\"relu\"),\n",
    "                                tf.keras.layers.Dropout(0.2),\n",
    "                                tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "                                #  tf.keras.layers.Dense(2, activation=\"softmax\")\n",
    "                                ])\n",
    "\n",
    "    lr_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                    patience=2, \n",
    "                                    factor=0.3, #0.5,\n",
    "                                    verbose=1, \n",
    "                                    min_lr=0.01) #0.01)\n",
    "    model.compile(optimizer = 'sgd',\n",
    "                loss = 'binary_crossentropy',\n",
    "                metrics = ['accuracy'])\n",
    "\n",
    "    #Train for 10 epochs\n",
    "    steps_per_epoch=int(len(train_generator)/batch_size)\n",
    "    validation_steps = validation_generator.samples // validation_generator.batch_size\n",
    "\n",
    "    # model.summary()\n",
    "    # summary_buffer = io.StringIO()\n",
    "\n",
    "    # # Redirect the stdout to the buffer and call model.summary()\n",
    "    # with redirect_stdout(summary_buffer):\n",
    "    #     model.summary()\n",
    "\n",
    "    # # Get the summary as a string\n",
    "    # summary = summary_buffer.getvalue()\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=[AccuracyLogger(  log_file, model_name, img_shape, batch_size, num_classes),\n",
    "                    EarlyStopping(patience=5)],\n",
    "        epochs=30,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    def plot_graphs(history, batch_size):\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12, 5))  # 1 row, 2 columns\n",
    "\n",
    "        # Plot accuracy\n",
    "        axs[0].plot(history.history['accuracy'])\n",
    "        axs[0].plot(history.history['val_accuracy'])\n",
    "        axs[0].set_title(\"Model Accuracy\")\n",
    "        axs[0].set_xlabel(\"Epochs\")\n",
    "        axs[0].set_ylabel(\"Accuracy\")\n",
    "        axs[0].legend(['Train', 'Validation'])\n",
    "\n",
    "        # Plot loss\n",
    "        axs[1].plot(history.history['loss'])\n",
    "        axs[1].plot(history.history['val_loss'])\n",
    "        axs[1].set_title(\"Model Loss\")\n",
    "        axs[1].set_xlabel(\"Epochs\")\n",
    "        axs[1].set_ylabel(\"Loss\")\n",
    "        axs[1].legend(['Train', 'Validation'])\n",
    "\n",
    "        plt.tight_layout()  # Adjust layout to make sure plots don't overlap\n",
    "\n",
    "        # Ensure the save directory exists\n",
    "        save_dir = '/app/data/graphs/'\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Create the filename using the dataset path and batch size\n",
    "        dataset_name = os.path.basename(dataset_path[29:])  # filter out '/app/data/Datasets/' \n",
    "        filename = f\"{dataset_name}_30epoch_batch{batch_size}_{model_name}.png\"\n",
    "\n",
    "        # Save the figure\n",
    "        save_path = os.path.join(save_dir, filename)\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Graph saved at: {save_path}\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # plot\n",
    "    plot_graphs(history, batch)\n",
    "\n",
    "    loss, accuracy = model.evaluate(test_generator, verbose=False)\n",
    "    print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f\"Testing Accuracy:  {accuracy:.4f}\\n\")\n",
    "        f.flush()\n",
    "\n",
    "    # save model\n",
    "    model.save(\"tf_model.keras\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
