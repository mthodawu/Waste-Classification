{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install scikit-learn\n",
    "# %pip install tensorflow\n",
    "# %pip install seaborn matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-07 15:13:15.326969: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-07 15:13:15.337005: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-07 15:13:15.348692: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-07 15:13:15.351957: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-07 15:13:15.360518: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3662 images belonging to 2 classes.\n",
      "Found 915 images belonging to 2 classes.\n",
      "Found 4577 images belonging to 2 classes.\n",
      "Found 2745 images belonging to 2 classes.\n",
      "Found 916 images belonging to 2 classes.\n",
      "Found 916 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728313998.720866   12076 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728313998.725035   12076 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728313998.725166   12076 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728313998.725685   12076 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728313998.725775   12076 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728313998.725837   12076 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728313998.776017   12076 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728313998.776123   12076 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728313998.776189   12076 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-07 15:13:18.776253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3478 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728314010.449047   12230 service.cc:146] XLA service 0x7593a00e02d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1728314010.449071   12230 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce GTX 1050 Ti, Compute Capability 6.1\n",
      "2024-10-07 15:13:30.883751: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-10-07 15:13:32.870819: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8906\n",
      "2024-10-07 15:13:44.723331: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.53GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Dense, Flatten\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, average_precision_score\n",
    "# Enable mixed precision training\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "clear_session()  # This frees the GPU memory, peventing training deadlock\n",
    "\n",
    "# Function to create directories\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "setz = ['metal','glass', 'cardboard', 'paper', 'plastic'] #,'trash']\n",
    "\n",
    "for setClass in setz: #setz:\n",
    "    clear_session()\n",
    "    global classez\n",
    "    classez = [setClass, 'other']\n",
    "    # data path\n",
    "    global dataset_path\n",
    "    dataset_path = f'/app/data/Datasets/augmented_{classez[0]}_and_{classez[1]}'  # <- docker file path      # \"G:\\My Drive\\Datasets\\ \"\n",
    "    \n",
    "\n",
    "    tar = (224, 224)\n",
    "    batch = 16 # 16\n",
    "    #Split the dataset\n",
    "    # Create an ImageDataGenerator for each subset\n",
    "    datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)  # 20% for validation\n",
    "\n",
    "    # Training data generator\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        dataset_path,\n",
    "        target_size=tar,\n",
    "        batch_size=batch,\n",
    "        class_mode='binary',\n",
    "        seed=42,\n",
    "        subset='training')  # Set as training data\n",
    "\n",
    "    # Validation data generator\n",
    "    validation_generator = datagen.flow_from_directory(\n",
    "        dataset_path,\n",
    "        target_size=tar,\n",
    "        batch_size=batch,\n",
    "        class_mode='binary',\n",
    "        seed=42,\n",
    "        subset='validation')  # Set as validation data\n",
    "\n",
    "    # Create a separate ImageDataGenerator for test data\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Test data generator\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        dataset_path,\n",
    "        target_size=tar,\n",
    "        batch_size=batch,\n",
    "        seed=42,\n",
    "        class_mode='binary')  # No subset for testing\n",
    "    \n",
    "   \n",
    "\n",
    "    # Paths to save splits\n",
    "    base_dir = f'/app/data/Datasets/augmented_{classez[0]}_and_{classez[1]}-split'  # \"G:\\My Drive\\Datasets\\Trashnet-resized-split\"\n",
    "    train_dir = os.path.join(base_dir, 'train')\n",
    "    val_dir = os.path.join(base_dir, 'val')\n",
    "    test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "    create_dir(base_dir)\n",
    "    create_dir(train_dir)\n",
    "    create_dir(val_dir)\n",
    "    create_dir(test_dir)\n",
    "\n",
    "    for class_name in classez: #, 'cardboard', 'metal', 'paper', 'plastic', 'trash']:\n",
    "        # Create class directories\n",
    "        create_dir(os.path.join(train_dir, class_name))\n",
    "        create_dir(os.path.join(val_dir, class_name))\n",
    "        create_dir(os.path.join(test_dir, class_name))\n",
    "\n",
    "        # List all files in the class directory\n",
    "        class_files = os.listdir(os.path.join(dataset_path, class_name))\n",
    "        train_files, test_files = train_test_split(class_files, test_size=0.2, random_state=42)  # 20% for testing\n",
    "        train_files, val_files = train_test_split(train_files, test_size=0.25, random_state=42)  # 20% of remaining for validation (0.25 * 0.8 = 0.2)\n",
    "\n",
    "        # Copy files to respective directories\n",
    "        for file in train_files:\n",
    "            shutil.copy(os.path.join(dataset_path, class_name, file), os.path.join(train_dir, class_name, file))\n",
    "\n",
    "        for file in val_files:\n",
    "            shutil.copy(os.path.join(dataset_path, class_name, file), os.path.join(val_dir, class_name, file))\n",
    "\n",
    "        for file in test_files:\n",
    "            shutil.copy(os.path.join(dataset_path, class_name, file), os.path.join(test_dir, class_name, file))\n",
    "        \n",
    "    train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=tar,\n",
    "    batch_size=batch,\n",
    "    seed=42,\n",
    "    class_mode='binary'\n",
    "    # ,color_mode='grayscale'\n",
    "    )\n",
    "\n",
    "    validation_generator = datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=tar,\n",
    "        batch_size=batch,\n",
    "        seed=42,\n",
    "        class_mode='binary'\n",
    "        # ,color_mode='grayscale'\n",
    "        )\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=tar,\n",
    "        batch_size=batch,\n",
    "        seed=42,\n",
    "        class_mode='binary'\n",
    "        # ,color_mode='grayscale'\n",
    "        )\n",
    "\n",
    "    img_shape = (224,224,3)\n",
    "    img_size = tar\n",
    "    batch_size = batch #16 #32\n",
    "    num_classes = 2\n",
    "    log_file = '/app/data/code/DenseNet201_log_100epochs.csv'\n",
    "\n",
    "    # DenseNet201 # MobileNetV3 # EfficientNetB0\n",
    "    base_model = tf.keras.applications.DenseNet201(input_shape=img_shape,\n",
    "                                                include_top=False,\n",
    "                                                weights='imagenet')\n",
    "\n",
    "    model_name = base_model.name #'EfficientNetB0' # 'DenseNet201' # 'MobileNetV2'\n",
    "    # print(model_name)\n",
    "\n",
    "    base_model.trainable = False\n",
    "    for layer in base_model.layers[-6:]:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    class AccuracyLogger(Callback):\n",
    "        def __init__(self,   log_file, model_name, img_shape, batch_size, num_classes):\n",
    "            super().__init__()\n",
    "            self.log_file = log_file\n",
    "            self.model_name = model_name\n",
    "            self.img_shape = img_shape\n",
    "            self.batch_size = batch_size\n",
    "            self.num_classes = num_classes\n",
    "            \n",
    "            # self.summary = summary\n",
    "\n",
    "            # Write model and dataset information at the beginning of the log file\n",
    "            with open(self.log_file, 'a') as f:\n",
    "                f.write(f\"\\nModel: {self.model_name}\\n\")\n",
    "                # f.write(f\"Model summary:{self.summary}\\n\")\n",
    "                f.write(f\"Image shape: {self.img_shape}\\n\")\n",
    "                f.write(f\"Batch size: {self.batch_size}\\n\")\n",
    "                f.write(f\"Dataset: {dataset_path}\\n\")\n",
    "                f.write(f\"Number of classes: {self.num_classes}\\n\\n\")\n",
    "                # f.write('Epoch,Accuracy,Validation Accuracy, Loss, Validation Loss\\n')\n",
    "                f.flush()\n",
    "\n",
    "        # def on_epoch_end(self, epoch, logs=None):\n",
    "        #     logs = logs or {}\n",
    "        #     accuracy = logs.get('accuracy')\n",
    "        #     val_accuracy = logs.get('val_accuracy')\n",
    "        #     loss = logs.get('loss')\n",
    "        #     val_loss = logs.get('val_loss')\n",
    "\n",
    "        #     # Write the epoch, accuracy, and validation accuracy to the log file\n",
    "        #     with open(self.log_file, 'a') as f:\n",
    "        #         f.write(f\"{epoch + 1},{accuracy},{val_accuracy},{loss},{val_loss}\\n\")\n",
    "        #         f.flush()\n",
    "\n",
    "    model = tf.keras.Sequential([base_model,\n",
    "                                #  tf.keras.layers.Flatten(),\n",
    "                                tf.keras.layers.GlobalAveragePooling2D(),\n",
    "                                #  tf.keras.layers.Dense(batch, activation=\"relu\"),\n",
    "                                tf.keras.layers.Dropout(0.2),\n",
    "                                tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "                                #  tf.keras.layers.Dense(2, activation=\"softmax\")\n",
    "                                ])\n",
    "\n",
    "    lr_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                    patience=2, \n",
    "                                    factor=0.3, #0.5,\n",
    "                                    verbose=1, \n",
    "                                    min_lr=0.01) #0.01)\n",
    "    model.compile(optimizer = 'sgd',\n",
    "                loss = 'binary_crossentropy',\n",
    "                metrics = ['accuracy'])\n",
    "\n",
    "    #Train for 10 epochs\n",
    "    steps_per_epoch=int(len(train_generator)/batch_size)\n",
    "    validation_steps = validation_generator.samples // validation_generator.batch_size\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=[AccuracyLogger(  log_file, model_name, img_shape, batch_size, num_classes),\n",
    "                    EarlyStopping(patience=5)],\n",
    "        epochs=100,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    def plot_graphs(history, batch_size):\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12, 5))  # 1 row, 2 columns\n",
    "\n",
    "        # Plot accuracy\n",
    "        axs[0].plot(history.history['accuracy'])\n",
    "        axs[0].plot(history.history['val_accuracy'])\n",
    "        axs[0].set_title(\"Model Accuracy\")\n",
    "        axs[0].set_xlabel(\"Epochs\")\n",
    "        axs[0].set_ylabel(\"Accuracy\")\n",
    "        axs[0].legend(['Train', 'Validation'])\n",
    "\n",
    "        # Plot loss\n",
    "        axs[1].plot(history.history['loss'])\n",
    "        axs[1].plot(history.history['val_loss'])\n",
    "        axs[1].set_title(\"Model Loss\")\n",
    "        axs[1].set_xlabel(\"Epochs\")\n",
    "        axs[1].set_ylabel(\"Loss\")\n",
    "        axs[1].legend(['Train', 'Validation'])\n",
    "\n",
    "        plt.tight_layout()  # Adjust layout to make sure plots don't overlap\n",
    "\n",
    "        # Ensure the save directory exists\n",
    "        save_dir = '/app/data/graphs/'\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Create the filename using the dataset path and batch size\n",
    "        dataset_name = os.path.basename(dataset_path[29:])  # filter out '/app/data/Datasets/' \n",
    "        filename = f\"{dataset_name}_100epoch_batch{batch_size}_{model_name}.png\"\n",
    "\n",
    "        # Save the figure\n",
    "        save_path = os.path.join(save_dir, filename)\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Graph saved at: {save_path}\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # plot\n",
    "    plot_graphs(history, batch)\n",
    "\n",
    "    loss, accuracy = model.evaluate(test_generator, verbose=False)\n",
    "    # Predict on test data\n",
    "    y_pred = model.predict(test_generator)\n",
    "    \n",
    "    threshold = 0.5\n",
    "    y_pred_classes = (y_pred >= threshold).astype(int)\n",
    "    # y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true = test_generator.classes\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred_classes, average='weighted')\n",
    "\n",
    "    # Calculate mAP\n",
    "    # Note: This assumes binary classification. For multi-class, you'll need to modify this.\n",
    "    mAP = average_precision_score(y_true, y_pred)\n",
    "\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "    # Calculate percentages for the confusion matrix\n",
    "    cm_percent = cm.astype('float') / cm.sum() * 100\n",
    "\n",
    "    # Create annotations that combine the raw values and percentages\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            annot[i, j] = f'{cm[i, j]}\\n({cm_percent[i, j]:.2f}%)'\n",
    "\n",
    "    \n",
    "    # Log the metrics\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f\"Testing Accuracy: {accuracy:.4f}\\n\")\n",
    "        f.write(f\"Precision: {precision:.4f}\\n\")\n",
    "        f.write(f\"Recall: {recall:.4f}\\n\")\n",
    "        f.write(f\"F1 Score: {f1:.4f}\\n\")\n",
    "        f.write(f\"mAP: {mAP:.4f}\\n\")\n",
    "        f.flush()\n",
    "\n",
    "    # Plot and save confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=annot, fmt='d', cmap='Blues', xticklabels=classez, yticklabels=classez)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(f'/app/data/graphs/{model_name}_{setClass}_confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Print metrics\n",
    "    # print(f\"Testing Accuracy: {accuracy:.4f}\")\n",
    "    # print(f\"Precision: {precision:.4f}\")\n",
    "    # print(f\"Recall: {recall:.4f}\")\n",
    "    # print(f\"F1 Score: {f1:.4f}\")\n",
    "    # print(f\"mAP: {mAP:.4f}\")\n",
    "    print(f\"Confusion Matrix saved as '{setClass}_confusion_matrix.png'\")\n",
    "\n",
    "    # Save model\n",
    "    model.save(f\"/app/data/{model_name}_{setClass}.keras\")\n",
    "    print(f\"{setClass} model trained and tested successfully!\")\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m precision_recall_fscore_support, confusion_matrix, average_precision_score\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#  Create confusion matrix\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(\u001b[43my_true\u001b[49m, y_pred_classes)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Calculate percentages for the confusion matrix\u001b[39;00m\n\u001b[1;32m      7\u001b[0m cm_percent \u001b[38;5;241m=\u001b[39m cm\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m/\u001b[39m cm\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_true' is not defined"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, average_precision_score\n",
    "\n",
    "# #  Create confusion matrix\n",
    "# cm = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "# # Calculate percentages for the confusion matrix\n",
    "# cm_percent = cm.astype('float') / cm.sum() * 100\n",
    "\n",
    "# # Create annotations that combine the raw values and percentages\n",
    "# annot = np.empty_like(cm).astype(str)\n",
    "# for i in range(cm.shape[0]):\n",
    "#     for j in range(cm.shape[1]):\n",
    "#         annot[i, j] = f'{cm[i, j]}\\n({cm_percent[i, j]:.2f}%)'\n",
    "\n",
    "\n",
    "# # Log the metrics\n",
    "# with open(log_file, 'a') as f:\n",
    "#     f.write(f\"Testing Accuracy: {accuracy:.4f}\\n\")\n",
    "#     f.write(f\"Precision: {precision:.4f}\\n\")\n",
    "#     f.write(f\"Recall: {recall:.4f}\\n\")\n",
    "#     f.write(f\"F1 Score: {f1:.4f}\\n\")\n",
    "#     f.write(f\"mAP: {mAP:.4f}\\n\")\n",
    "#     f.flush()\n",
    "\n",
    "# # Plot and save confusion matrix\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(cm, annot=annot, fmt='d', cmap='Blues', xticklabels=classez, yticklabels=classez)\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.ylabel('True Label')\n",
    "# plt.xlabel('Predicted Label')\n",
    "# plt.savefig(f'/app/data/graphs/{model_name}_{setClass}_confusion_matrix.png')\n",
    "# plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
