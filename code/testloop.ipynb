{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3662 images belonging to 2 classes.\n",
      "Found 915 images belonging to 2 classes.\n",
      "Found 4577 images belonging to 2 classes.\n",
      "Found 4026 images belonging to 2 classes.\n",
      "Found 1006 images belonging to 2 classes.\n",
      "Found 5032 images belonging to 2 classes.\n",
      "Found 2570 images belonging to 2 classes.\n",
      "Found 642 images belonging to 2 classes.\n",
      "Found 3212 images belonging to 2 classes.\n",
      "Found 3635 images belonging to 2 classes.\n",
      "Found 907 images belonging to 2 classes.\n",
      "Found 4542 images belonging to 2 classes.\n",
      "Found 4399 images belonging to 2 classes.\n",
      "Found 1098 images belonging to 2 classes.\n",
      "Found 5497 images belonging to 2 classes.\n",
      "Found 3950 images belonging to 2 classes.\n",
      "Found 987 images belonging to 2 classes.\n",
      "Found 4937 images belonging to 2 classes.\n",
      "Found 2961 images belonging to 2 classes.\n",
      "Found 2961 images belonging to 2 classes.\n",
      "Found 988 images belonging to 2 classes.\n",
      "Found 988 images belonging to 2 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m74836368/74836368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 0us/step\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-18 11:37:03.201726: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:536] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\n",
      "2024-09-18 11:37:03.201764: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:540] Memory usage: 11468800 bytes free, 4227727360 bytes total.\n",
      "2024-09-18 11:37:03.201804: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:550] Possibly insufficient driver version: 555.58.2\n",
      "2024-09-18 11:37:03.411036: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:536] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\n",
      "2024-09-18 11:37:03.411086: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:540] Memory usage: 11468800 bytes free, 4227727360 bytes total.\n",
      "2024-09-18 11:37:03.411115: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:550] Possibly insufficient driver version: 555.58.2\n",
      "2024-09-18 11:37:03.458739: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at xla_ops.cc:577 : FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.\n",
      "2024-09-18 11:37:03.458806: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.\n",
      "\t [[{{node StatefulPartitionedCall}}]]\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/usr/lib/python3.11/runpy.py\", line 198, in _run_module_as_main\n\n  File \"/usr/lib/python3.11/runpy.py\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 604, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_42203/477148227.py\", line 217, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 318, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n\nDNN library initialization failed. Look at the errors above for more details.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_one_step_on_iterator_43835]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 217\u001b[0m\n\u001b[1;32m    204\u001b[0m validation_steps \u001b[38;5;241m=\u001b[39m validation_generator\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m validation_generator\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# model.summary()\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# summary_buffer = io.StringIO()\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m \n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mAccuracyLogger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[43mlog_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m                \u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    225\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_graphs\u001b[39m(history, batch_size):\n\u001b[1;32m    228\u001b[0m     fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m5\u001b[39m))  \u001b[38;5;66;03m# 1 row, 2 columns\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/usr/lib/python3.11/runpy.py\", line 198, in _run_module_as_main\n\n  File \"/usr/lib/python3.11/runpy.py\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 604, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_42203/477148227.py\", line 217, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 318, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n\nDNN library initialization failed. Look at the errors above for more details.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_one_step_on_iterator_43835]"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Dense, Flatten\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Enable mixed precision training\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for set in ['metal','glass', 'trash', 'cardboard', 'paper', 'plastic']:\n",
    "    clear_session()  # This frees the GPU memory, peventing training deadlock\n",
    "    global classez\n",
    "    classez = [set, 'other']\n",
    "    # data path\n",
    "    global dataset_path\n",
    "    dataset_path = f'/app/data/Datasets/augmented_{classez[0]}_and_{classez[1]}'  # <- docker file path      # \"G:\\My Drive\\Datasets\\ \"\n",
    "\n",
    "    tar = (224, 224)\n",
    "    batch = 16 # 16\n",
    "    #Split the dataset\n",
    "    # Create an ImageDataGenerator for each subset\n",
    "    datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)  # 20% for validation\n",
    "\n",
    "    # Training data generator\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        dataset_path,\n",
    "        target_size=tar,\n",
    "        batch_size=batch,\n",
    "        class_mode='binary',\n",
    "        subset='training')  # Set as training data\n",
    "\n",
    "    # Validation data generator\n",
    "    validation_generator = datagen.flow_from_directory(\n",
    "        dataset_path,\n",
    "        target_size=tar,\n",
    "        batch_size=batch,\n",
    "        class_mode='binary',\n",
    "        subset='validation')  # Set as validation data\n",
    "\n",
    "    # Create a separate ImageDataGenerator for test data\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Test data generator\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        dataset_path,\n",
    "        target_size=tar,\n",
    "        batch_size=batch,\n",
    "        class_mode='binary')  # No subset for testing\n",
    "    \n",
    "    # Function to create directories\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "# Paths to save splits\n",
    "base_dir = f'/app/data/Datasets/augmented_{classez[0]}_and_{classez[1]}-split'  # \"G:\\My Drive\\Datasets\\Trashnet-resized-split\"\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "val_dir = os.path.join(base_dir, 'val')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "create_dir(base_dir)\n",
    "create_dir(train_dir)\n",
    "create_dir(val_dir)\n",
    "create_dir(test_dir)\n",
    "\n",
    "for class_name in classez: #, 'cardboard', 'metal', 'paper', 'plastic', 'trash']:\n",
    "    # Create class directories\n",
    "    create_dir(os.path.join(train_dir, class_name))\n",
    "    create_dir(os.path.join(val_dir, class_name))\n",
    "    create_dir(os.path.join(test_dir, class_name))\n",
    "\n",
    "    # List all files in the class directory\n",
    "    class_files = os.listdir(os.path.join(dataset_path, class_name))\n",
    "    train_files, test_files = train_test_split(class_files, test_size=0.2, random_state=42)  # 20% for testing\n",
    "    train_files, val_files = train_test_split(train_files, test_size=0.25, random_state=42)  # 20% of remaining for validation (0.25 * 0.8 = 0.2)\n",
    "\n",
    "    # Copy files to respective directories\n",
    "    for file in train_files:\n",
    "        shutil.copy(os.path.join(dataset_path, class_name, file), os.path.join(train_dir, class_name, file))\n",
    "\n",
    "    for file in val_files:\n",
    "        shutil.copy(os.path.join(dataset_path, class_name, file), os.path.join(val_dir, class_name, file))\n",
    "\n",
    "    for file in test_files:\n",
    "        shutil.copy(os.path.join(dataset_path, class_name, file), os.path.join(test_dir, class_name, file))\n",
    "    \n",
    "    train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=tar,\n",
    "    batch_size=batch,\n",
    "    class_mode='binary'\n",
    "    # ,color_mode='grayscale'\n",
    "    )\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=tar,\n",
    "    batch_size=batch,\n",
    "    class_mode='binary'\n",
    "    # ,color_mode='grayscale'\n",
    "    )\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=tar,\n",
    "    batch_size=batch,\n",
    "    class_mode='binary'\n",
    "    # ,color_mode='grayscale'\n",
    "    )\n",
    "\n",
    "img_shape = (224,224,3)\n",
    "img_size = tar\n",
    "batch_size = batch #16 #32\n",
    "num_classes = 2\n",
    "log_file = '/app/data/code/accuracy_log7.csv'\n",
    "\n",
    "# DenseNet201 # MobileNetV3 # EfficientNetB0\n",
    "base_model = tf.keras.applications.DenseNet201(input_shape=img_shape,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "\n",
    "model_name = base_model.name #'EfficientNetB0' # 'DenseNet201' # 'MobileNetV2'\n",
    "# print(model_name)\n",
    "\n",
    "base_model.trainable = False\n",
    "for layer in base_model.layers[-6:]:\n",
    "     layer.trainable = True\n",
    "    \n",
    "class AccuracyLogger(Callback):\n",
    "    def __init__(self,   log_file, model_name, img_shape, batch_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.log_file = log_file\n",
    "        self.model_name = model_name\n",
    "        self.img_shape = img_shape\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # self.summary = summary\n",
    "\n",
    "        # Write model and dataset information at the beginning of the log file\n",
    "        with open(self.log_file, 'a') as f:\n",
    "            f.write(f\"\\nModel: {self.model_name}\\n\")\n",
    "            # f.write(f\"Model summary:{self.summary}\\n\")\n",
    "            f.write(f\"Image shape: {self.img_shape}\\n\")\n",
    "            f.write(f\"Batch size: {self.batch_size}\\n\")\n",
    "            f.write(f\"Dataset: {dataset_path}\\n\")\n",
    "            f.write(f\"Number of classes: {self.num_classes}\\n\\n\")\n",
    "            f.write('Epoch,Accuracy,Validation Accuracy, Loss, Validation Loss\\n')\n",
    "            f.flush()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        accuracy = logs.get('accuracy')\n",
    "        val_accuracy = logs.get('val_accuracy')\n",
    "        loss = logs.get('loss')\n",
    "        val_loss = logs.get('val_loss')\n",
    "\n",
    "        # Write the epoch, accuracy, and validation accuracy to the log file\n",
    "        with open(self.log_file, 'a') as f:\n",
    "            f.write(f\"{epoch + 1},{accuracy},{val_accuracy},{loss},{val_loss}\\n\")\n",
    "            f.flush()\n",
    "\n",
    "# import io\n",
    "# from contextlib import redirect_stdout\n",
    "model = tf.keras.Sequential([base_model,\n",
    "                            #  tf.keras.layers.Flatten(),\n",
    "                             tf.keras.layers.GlobalAveragePooling2D(),\n",
    "                            #  tf.keras.layers.Dense(batch, activation=\"relu\"),\n",
    "                             tf.keras.layers.Dropout(0.2),\n",
    "                             tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "                            #  tf.keras.layers.Dense(2, activation=\"softmax\")\n",
    "                             ])\n",
    "\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                 patience=2, \n",
    "                                 factor=0.3, #0.5,\n",
    "                                 verbose=1, \n",
    "                                 min_lr=0.01) #0.01)\n",
    "model.compile(optimizer = 'sgd',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "#Train for 10 epochs\n",
    "steps_per_epoch=int(len(train_generator)/batch_size)\n",
    "validation_steps = validation_generator.samples // validation_generator.batch_size\n",
    "\n",
    "# model.summary()\n",
    "# summary_buffer = io.StringIO()\n",
    "\n",
    "# # Redirect the stdout to the buffer and call model.summary()\n",
    "# with redirect_stdout(summary_buffer):\n",
    "#     model.summary()\n",
    "\n",
    "# # Get the summary as a string\n",
    "# summary = summary_buffer.getvalue()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[AccuracyLogger(  log_file, model_name, img_shape, batch_size, num_classes),\n",
    "                EarlyStopping(patience=5)],\n",
    "    epochs=30,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "def plot_graphs(history, batch_size):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))  # 1 row, 2 columns\n",
    "\n",
    "    # Plot accuracy\n",
    "    axs[0].plot(history.history['accuracy'])\n",
    "    axs[0].plot(history.history['val_accuracy'])\n",
    "    axs[0].set_title(\"Model Accuracy\")\n",
    "    axs[0].set_xlabel(\"Epochs\")\n",
    "    axs[0].set_ylabel(\"Accuracy\")\n",
    "    axs[0].legend(['Train', 'Validation'])\n",
    "\n",
    "    # Plot loss\n",
    "    axs[1].plot(history.history['loss'])\n",
    "    axs[1].plot(history.history['val_loss'])\n",
    "    axs[1].set_title(\"Model Loss\")\n",
    "    axs[1].set_xlabel(\"Epochs\")\n",
    "    axs[1].set_ylabel(\"Loss\")\n",
    "    axs[1].legend(['Train', 'Validation'])\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to make sure plots don't overlap\n",
    "\n",
    "    # Ensure the save directory exists\n",
    "    save_dir = '/app/data/graphs/'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Create the filename using the dataset path and batch size\n",
    "    dataset_name = os.path.basename(dataset_path[29:])  # filter out '/app/data/Datasets/' \n",
    "    filename = f\"{dataset_name}_30epoch_batch{batch_size}_{model_name}.png\"\n",
    "\n",
    "    # Save the figure\n",
    "    save_path = os.path.join(save_dir, filename)\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"Graph saved at: {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# plot\n",
    "plot_graphs(history, batch)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_generator, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "with open(log_file, 'a') as f:\n",
    "    f.write(f\"Testing Accuracy:  {accuracy:.4f}\\n\")\n",
    "    f.flush()\n",
    "\n",
    "# save model\n",
    "model.save(\"tf_model.keras\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
